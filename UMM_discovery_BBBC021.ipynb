{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Copyright 2020 Novartis Institutes for BioMedical Research Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "This file incorporates work covered by the following copyright and  \n",
    "permission notice:\n",
    "\n",
    "   Copyright (c) 2017-present, Facebook, Inc.\n",
    "   All rights reserved.\n",
    "\n",
    "   This source code is licensed as Creative Commons Attribution-Noncommercial \n",
    "   and can be found under https://creativecommons.org/licenses/by-nc/4.0/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMM Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU device\n",
    "import os\n",
    "print(os.getenv('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning, NumbaPerformanceWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPerformanceWarning)\n",
    "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/umm_discovery_env/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib==1.1.0 in /opt/conda/envs/umm_discovery_env/lib/python3.7/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import clustering\n",
    "from model import MultiScaleNet\n",
    "from util import Logger, UnifLabelSampler\n",
    "from training import compute_features, train\n",
    "from correction import do_batch_correction\n",
    "from evaluation import evaluate_epoch, evaluate_training\n",
    "from my_dataset import bbbc021_dataset\n",
    "from plot import plot_training, plot_training_summary, plot_inter_part_validation, plot_inter_hier_validation, plot_external_validation, plot_NSC_NSB, plot_total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    # Experiment ID\n",
    "    run_ID = \"EXP01\"\n",
    "    \n",
    "    # path to dataset\n",
    "    data = \"/UMM-Discovery/data\"\n",
    "    \n",
    "    # file name of the metadata csv file\n",
    "    csv = \"UMM_meta.csv\"\n",
    "    \n",
    "    # path to the output folder\n",
    "    save_path = \"/UMM-Discovery/out\"\n",
    "    output_path = \"\" #dont assign\n",
    "\n",
    "    # CNN architecture \n",
    "    arch = \"msnet\" \n",
    "\n",
    "    # input channels\n",
    "    n_input_dim = 3\n",
    "    \n",
    "    # number of features\n",
    "    n_features = 64\n",
    "    \n",
    "    # Batch correction [\"TVN\", \"COMBAT\", MNN]\n",
    "    batch_corrections = [\"COMBAT\", \"TVN\"] \n",
    "    \n",
    "    # dimension reduction method (default: PCA)\n",
    "    # 'PCA' or 'UMAP' or 'TSNE' or 'AdaptiveTSNE'\n",
    "    dim_method = 'PCA' \n",
    "    \n",
    "    # dimensions after reduction\n",
    "    n_components = 16\n",
    "    \n",
    "    # clustering algorithm (default: Kmeans)\n",
    "    # 'Kmeans' or 'AdaptiveKmeans' or 'PIC' or 'HDBscan' \n",
    "    clustering = 'Kmeans'\n",
    "\n",
    "    # number of cluster for k-means (default: 10000)\n",
    "    nmb_cluster = 104\n",
    "    \n",
    "    # parameter for adaptive k-means\n",
    "    end_k = 39\n",
    "    end_epochs = 150\n",
    "    \n",
    "    # parameter for PIC\n",
    "    sigma = 0.2 \n",
    "    nnn = 5\n",
    "    \n",
    "    # paramter for HDBscan\n",
    "    min_cluster_size = 12\n",
    "    min_samples = 3\n",
    "    nnn = 25\n",
    "    \n",
    "    # mini-batch size (default: 256)\n",
    "    batch = 16\n",
    "    \n",
    "    # learning rate (default: 0.05)\n",
    "    lr = 0.05\n",
    "    \n",
    "    # weight decay pow (default: -5)\n",
    "    wd=-5\n",
    "    \n",
    "    # number of total epochs to run (default: 200)\n",
    "    epochs = 50\n",
    "\n",
    "    # momentum (default: 0.9)\n",
    "    momentum = 0.9\n",
    "\n",
    "    # how many epochs of training between two consecutive reassignments of clusters (default: 1)\n",
    "    reassign = 1\n",
    "\n",
    "    # number of data loading workers (default: 4)\n",
    "    workers=4\n",
    "    \n",
    "    # cuda version\n",
    "    cuda = 10\n",
    "    \n",
    "    # Epoch to continue from (default: None)\n",
    "    resume = None\n",
    "    \n",
    "    # manual epoch number (useful on restarts) (default: 0)\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # random seed (default: 42)\n",
    "    seed = 12\n",
    "    \n",
    "    # chatty\n",
    "    verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: EXP01_Kmeans_cl104_PCA-16d_COMBAT_TVN\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"\"\n",
    "exp_name +='{}_'.format(args.run_ID)\n",
    "\n",
    "## Add clustering algorithm\n",
    "if args.clustering == \"Kmeans\":\n",
    "    clustering_parameter = 'Kmeans_cl{}'.format(args.nmb_cluster)\n",
    "elif args.clustering == \"AdaptiveKmeans\":\n",
    "    clustering_parameter = 'AdaptiveKmeans_cl{}-{}_ep{}'.format(args.nmb_cluster, args.end_k, args.end_epochs)\n",
    "elif args.clustering == \"PIC\":\n",
    "    clustering_parameter = 'PIC_sig{}_nn{}'.format(args.sigma, args.nnn)\n",
    "elif args.clustering == \"HDBscan\":\n",
    "    clustering_parameter = 'HDBscan_mincl{}-minsam{}_nn{}'.format(args.min_cluster_size, args.min_samples, args.nnn)\n",
    "\n",
    "exp_name +='{}'.format(clustering_parameter)\n",
    "    \n",
    "## Add dimensionality reduction method\n",
    "exp_name +='_{}-{}d'.format(args.dim_method, args.n_components)\n",
    "\n",
    "## Add batch correction\n",
    "batch_corrections_parameter = None\n",
    "if args.batch_corrections == None:\n",
    "    batch_corrections_parameter = \"NoCorrection\"\n",
    "else:\n",
    "    for corr in args.batch_corrections:\n",
    "        if batch_corrections_parameter is None:\n",
    "            batch_corrections_parameter = corr\n",
    "        else:\n",
    "            batch_corrections_parameter +='_{}'.format(corr)\n",
    "exp_name +='_{}'.format(batch_corrections_parameter)\n",
    "    \n",
    "print('Experiment: {}'.format(exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output path: /path/to/output/folder/EXP01_Kmeans_cl104_PCA-16d_COMBAT_TVN\n"
     ]
    }
   ],
   "source": [
    "args.output_path = os.path.join(args.save_path, exp_name)\n",
    "print('Output path: {}'.format(args.output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path = os.path.join(args.output_path, 'y-embed_{}.txt'.format(exp_name))\n",
    "metadata_path = os.path.join(args.output_path, 'metadata_{}.txt'.format(exp_name))\n",
    "assign_epoch_path = os.path.join(args.output_path, 'epoch_assignment_{}.txt'.format(exp_name))\n",
    "metrices_path = os.path.join(args.output_path, 'metrices_{}.txt'.format(exp_name))\n",
    "loss_path = os.path.join(args.output_path, 'loss_{}.txt'.format(exp_name))\n",
    "\n",
    "best_labeled_sil_embed_path = os.path.join(args.output_path, 'best_labeled_sil_y-embed_{}.txt'.format(exp_name))\n",
    "best_labeled_nsc_embed_path = os.path.join(args.output_path, 'best_labeled_nsc_y-embed_{}.txt'.format(exp_name))\n",
    "best_unlabeled_embed_path = os.path.join(args.output_path, 'best_unlabeled_y-embed_{}.txt'.format(exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_cols = ['Z{:03d}'.format(i) for i in range(args.n_features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Multi-Scale Neural Network\n"
     ]
    }
   ],
   "source": [
    "# fix random seeds\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# CNN\n",
    "if args.verbose:\n",
    "    print('Create Multi-Scale Neural Network')\n",
    "    \n",
    "model = MultiScaleNet(input_dim=3, num_features=args.n_features, num_classes=args.nmb_cluster)\n",
    "fd = int(model.top_layer.weight.size()[1])\n",
    "model.top_layer = None\n",
    "model.features = torch.nn.DataParallel(model.features)\n",
    "model.cuda()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    filter(lambda x: x.requires_grad, model.parameters()),\n",
    "    lr=args.lr,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=10**args.wd,\n",
    ")\n",
    "\n",
    "# define loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# optionally resume from a checkpoint\n",
    "if args.resume:\n",
    "    checkpoint_path = os.path.join(args.output_path, 'checkpoints', 'checkpoint_epoch{}.pth.tar'.format(args.resume))\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(\"=> loading checkpoint '{}'\".format(checkpoint_path))\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        # remove top_layer parameters from checkpoint\n",
    "        for key in list(checkpoint['state_dict']):\n",
    "            if 'top_layer' in key:\n",
    "                del checkpoint['state_dict'][key]\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(args.resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "# creating checkpoint repo\n",
    "exp_check = os.path.join(args.output_path, 'checkpoints')\n",
    "if not os.path.isdir(exp_check):\n",
    "    os.makedirs(exp_check)\n",
    "\n",
    "# creating cluster assignments log\n",
    "cluster_log = Logger(os.path.join(args.output_path, 'train_data.pkl'))\n",
    "if args.resume:\n",
    "    cluster_log.load_data(args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         MaxPool2d-1          [-1, 3, 256, 320]               0\n",
      "            Conv2d-2          [-1, 6, 512, 640]             168\n",
      "       BatchNorm2d-3          [-1, 6, 512, 640]              12\n",
      "              ReLU-4          [-1, 6, 512, 640]               0\n",
      "            Conv2d-5          [-1, 9, 512, 640]             495\n",
      "       BatchNorm2d-6          [-1, 9, 512, 640]              18\n",
      "              ReLU-7          [-1, 9, 512, 640]               0\n",
      "            Conv2d-8          [-1, 9, 512, 640]              90\n",
      "       BatchNorm2d-9          [-1, 9, 512, 640]              18\n",
      "             ReLU-10          [-1, 9, 512, 640]               0\n",
      "        MaxPool2d-11          [-1, 9, 256, 320]               0\n",
      "           Conv2d-12         [-1, 12, 256, 320]             156\n",
      "      BatchNorm2d-13         [-1, 12, 256, 320]              24\n",
      "             ReLU-14         [-1, 12, 256, 320]               0\n",
      "          MSBlock-15         [-1, 12, 256, 320]               0\n",
      "        MaxPool2d-16         [-1, 12, 128, 160]               0\n",
      "           Conv2d-17         [-1, 12, 256, 320]           1,308\n",
      "      BatchNorm2d-18         [-1, 12, 256, 320]              24\n",
      "             ReLU-19         [-1, 12, 256, 320]               0\n",
      "           Conv2d-20         [-1, 20, 256, 320]           2,180\n",
      "      BatchNorm2d-21         [-1, 20, 256, 320]              40\n",
      "             ReLU-22         [-1, 20, 256, 320]               0\n",
      "           Conv2d-23         [-1, 20, 256, 320]             420\n",
      "      BatchNorm2d-24         [-1, 20, 256, 320]              40\n",
      "             ReLU-25         [-1, 20, 256, 320]               0\n",
      "        MaxPool2d-26         [-1, 20, 128, 160]               0\n",
      "           Conv2d-27         [-1, 32, 128, 160]           1,056\n",
      "      BatchNorm2d-28         [-1, 32, 128, 160]              64\n",
      "             ReLU-29         [-1, 32, 128, 160]               0\n",
      "          MSBlock-30         [-1, 32, 128, 160]               0\n",
      "        MaxPool2d-31           [-1, 32, 64, 80]               0\n",
      "           Conv2d-32         [-1, 16, 128, 160]           4,624\n",
      "      BatchNorm2d-33         [-1, 16, 128, 160]              32\n",
      "             ReLU-34         [-1, 16, 128, 160]               0\n",
      "           Conv2d-35         [-1, 32, 128, 160]           4,640\n",
      "      BatchNorm2d-36         [-1, 32, 128, 160]              64\n",
      "             ReLU-37         [-1, 32, 128, 160]               0\n",
      "           Conv2d-38         [-1, 32, 128, 160]           1,056\n",
      "      BatchNorm2d-39         [-1, 32, 128, 160]              64\n",
      "             ReLU-40         [-1, 32, 128, 160]               0\n",
      "        MaxPool2d-41           [-1, 32, 64, 80]               0\n",
      "           Conv2d-42           [-1, 64, 64, 80]           4,160\n",
      "      BatchNorm2d-43           [-1, 64, 64, 80]             128\n",
      "             ReLU-44           [-1, 64, 64, 80]               0\n",
      "          MSBlock-45           [-1, 64, 64, 80]               0\n",
      "        MaxPool2d-46           [-1, 64, 32, 40]               0\n",
      "           Conv2d-47           [-1, 16, 64, 80]           9,232\n",
      "      BatchNorm2d-48           [-1, 16, 64, 80]              32\n",
      "             ReLU-49           [-1, 16, 64, 80]               0\n",
      "           Conv2d-50           [-1, 32, 64, 80]           4,640\n",
      "      BatchNorm2d-51           [-1, 32, 64, 80]              64\n",
      "             ReLU-52           [-1, 32, 64, 80]               0\n",
      "           Conv2d-53           [-1, 32, 64, 80]           1,056\n",
      "      BatchNorm2d-54           [-1, 32, 64, 80]              64\n",
      "             ReLU-55           [-1, 32, 64, 80]               0\n",
      "        MaxPool2d-56           [-1, 32, 32, 40]               0\n",
      "           Conv2d-57           [-1, 96, 32, 40]           9,312\n",
      "      BatchNorm2d-58           [-1, 96, 32, 40]             192\n",
      "             ReLU-59           [-1, 96, 32, 40]               0\n",
      "          MSBlock-60           [-1, 96, 32, 40]               0\n",
      "        MaxPool2d-61           [-1, 96, 16, 20]               0\n",
      "           Conv2d-62           [-1, 16, 32, 40]          13,840\n",
      "      BatchNorm2d-63           [-1, 16, 32, 40]              32\n",
      "             ReLU-64           [-1, 16, 32, 40]               0\n",
      "           Conv2d-65           [-1, 32, 32, 40]           4,640\n",
      "      BatchNorm2d-66           [-1, 32, 32, 40]              64\n",
      "             ReLU-67           [-1, 32, 32, 40]               0\n",
      "           Conv2d-68           [-1, 32, 32, 40]           1,056\n",
      "      BatchNorm2d-69           [-1, 32, 32, 40]              64\n",
      "             ReLU-70           [-1, 32, 32, 40]               0\n",
      "        MaxPool2d-71           [-1, 32, 16, 20]               0\n",
      "           Conv2d-72          [-1, 128, 16, 20]          16,512\n",
      "      BatchNorm2d-73          [-1, 128, 16, 20]             256\n",
      "             ReLU-74          [-1, 128, 16, 20]               0\n",
      "          MSBlock-75          [-1, 128, 16, 20]               0\n",
      "AdaptiveAvgPool2d-76            [-1, 128, 1, 1]               0\n",
      "AdaptiveMaxPool2d-77            [-1, 128, 1, 1]               0\n",
      "         Collapse-78            [-1, 128, 1, 1]             128\n",
      "            Dense-79            [-1, 128, 1, 1]          16,384\n",
      "      BatchNorm2d-80            [-1, 128, 1, 1]             256\n",
      "             ReLU-81            [-1, 128, 1, 1]               0\n",
      "     DataParallel-82            [-1, 128, 1, 1]               0\n",
      "           Linear-83                   [-1, 64]           8,256\n",
      "             ReLU-84                   [-1, 64]               0\n",
      "================================================================\n",
      "Total params: 106,961\n",
      "Trainable params: 106,961\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.75\n",
      "Forward/backward pass size (MB): 407.98\n",
      "Params size (MB): 0.41\n",
      "Estimated Total Size (MB): 412.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if args.verbose:\n",
    "    summary(model, input_size=(3, 512, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda==10:\n",
    "    # create tensorboard writer\n",
    "    tensorboard_path = os.path.join(args.save_path, 'runs', exp_name)\n",
    "\n",
    "    if os.path.isdir(tensorboard_path) and not args.resume:\n",
    "        shutil.rmtree(tensorboard_path)\n",
    "\n",
    "    if not os.path.isdir(tensorboard_path):\n",
    "        os.makedirs(tensorboard_path)\n",
    "    \n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    tensorboard_writer = SummaryWriter(log_dir=tensorboard_path)\n",
    "else:\n",
    "    tensorboard_writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = args.data\n",
    "metadata = args.csv\n",
    "shabi = pd.read_csv(os.path.join(root_dir, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "shabi = pd.read_csv(\"UMM_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in shabi.keys():\n",
    "    shabi[k]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_nr</th>\n",
       "      <th>ImageNumber</th>\n",
       "      <th>filename_dna</th>\n",
       "      <th>plate</th>\n",
       "      <th>filename_tubulin</th>\n",
       "      <th>Image_PathName_Tubulin</th>\n",
       "      <th>filename_actin</th>\n",
       "      <th>Image_PathName_Actin</th>\n",
       "      <th>Image_Metadata_Plate_DAPI</th>\n",
       "      <th>Image_Metadata_Well_DAPI</th>\n",
       "      <th>Replicate</th>\n",
       "      <th>compound</th>\n",
       "      <th>compound_uM</th>\n",
       "      <th>MoA</th>\n",
       "      <th>Unique_MoA</th>\n",
       "      <th>Unique_Treatments</th>\n",
       "      <th>Unique_Compounds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Week1_150607_B04_s1_w11323931B-BDA7-4F42-870E-...</td>\n",
       "      <td>Week1_22123</td>\n",
       "      <td>Week1_150607_B04_s1_w2F8F7EA7A-EC57-49CA-A556-...</td>\n",
       "      <td>Week1/Week1_22123</td>\n",
       "      <td>Week1_150607_B04_s1_w494DCA5C4-3531-497D-A8B0-...</td>\n",
       "      <td>Week1/Week1_22123</td>\n",
       "      <td>Week1_22123</td>\n",
       "      <td>B04</td>\n",
       "      <td>1</td>\n",
       "      <td>cytochalasin B</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Actin disruptors</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Week1_150607_B04_s2_w1F649C703-6FA8-406F-8575-...</td>\n",
       "      <td>Week1_22123</td>\n",
       "      <td>Week1_150607_B04_s2_w2802CC81E-56F9-41C5-A6C2-...</td>\n",
       "      <td>Week1/Week1_22123</td>\n",
       "      <td>Week1_150607_B04_s2_w4342F300D-60F8-4256-A637-...</td>\n",
       "      <td>Week1/Week1_22123</td>\n",
       "      <td>Week1_22123</td>\n",
       "      <td>B04</td>\n",
       "      <td>1</td>\n",
       "      <td>cytochalasin B</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Actin disruptors</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Week1_150607_B04_s3_w135D66B4C-0548-4AB8-A57B-...</td>\n",
       "      <td>Week1_22123</td>\n",
       "      <td>Week1_150607_B04_s3_w2D81AEFB9-9DD4-4B59-9177-...</td>\n",
       "      <td>Week1/Week1_22123</td>\n",
       "      <td>Week1_150607_B04_s3_w408BE006A-BF34-457E-81A9-...</td>\n",
       "      <td>Week1/Week1_22123</td>\n",
       "      <td>Week1_22123</td>\n",
       "      <td>B04</td>\n",
       "      <td>1</td>\n",
       "      <td>cytochalasin B</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Actin disruptors</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>Week1_150607_B04_s4_w1EB720AD1-83BA-48A7-8C47-...</td>\n",
       "      <td>Week1_22123</td>\n",
       "      <td>Week1_150607_B04_s4_w261B79A05-7534-46F3-8C80-...</td>\n",
       "      <td>Week1/Week1_22123</td>\n",
       "      <td>Week1_150607_B04_s4_w49FF7E7B1-F049-4994-BCA2-...</td>\n",
       "      <td>Week1/Week1_22123</td>\n",
       "      <td>Week1_22123</td>\n",
       "      <td>B04</td>\n",
       "      <td>1</td>\n",
       "      <td>cytochalasin B</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Actin disruptors</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>249</td>\n",
       "      <td>Week1_150607_B04_s1_w17D5A011C-EA14-4EA7-A62C-...</td>\n",
       "      <td>Week1_22141</td>\n",
       "      <td>Week1_150607_B04_s1_w20ED4F2D9-0D4A-4CA6-A13D-...</td>\n",
       "      <td>Week1/Week1_22141</td>\n",
       "      <td>Week1_150607_B04_s1_w413EE84DD-212B-4008-99A9-...</td>\n",
       "      <td>Week1/Week1_22141</td>\n",
       "      <td>Week1_22141</td>\n",
       "      <td>B04</td>\n",
       "      <td>2</td>\n",
       "      <td>cytochalasin B</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Actin disruptors</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>9</td>\n",
       "      <td>3560</td>\n",
       "      <td>Week9_090907_F11_s4_w19580FF4D-DC3D-4BD0-93FE-...</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>Week9_090907_F11_s4_w2DC65EC6F-BDCA-4B05-B243-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_090907_F11_s4_w45699A0F4-9AEE-4CD4-8973-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>F11</td>\n",
       "      <td>3</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3844</th>\n",
       "      <td>9</td>\n",
       "      <td>3597</td>\n",
       "      <td>Week9_090907_G11_s1_w1EDE534D2-FCEE-4F92-A30B-...</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>Week9_090907_G11_s1_w26A22E27F-6A81-43F5-9587-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_090907_G11_s1_w4554A2BF7-0D53-4D27-BF92-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>G11</td>\n",
       "      <td>3</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>9</td>\n",
       "      <td>3598</td>\n",
       "      <td>Week9_090907_G11_s2_w10B010F39-3B4B-4DCB-8E34-...</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>Week9_090907_G11_s2_w2720AC778-3F85-4293-8D75-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_090907_G11_s2_w49B290958-BCF2-4DDD-B0E9-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>G11</td>\n",
       "      <td>3</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>9</td>\n",
       "      <td>3599</td>\n",
       "      <td>Week9_090907_G11_s3_w10394282C-6D3D-4E0E-9FA3-...</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>Week9_090907_G11_s3_w24C59DB62-E99B-4284-BAD2-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_090907_G11_s3_w471FE25C8-2477-456F-9D74-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>G11</td>\n",
       "      <td>3</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>9</td>\n",
       "      <td>3600</td>\n",
       "      <td>Week9_090907_G11_s4_w1C447A151-1F85-4E19-9C96-...</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>Week9_090907_G11_s4_w22E574F48-321D-4470-ACC4-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_090907_G11_s4_w4200C5003-7F75-47DF-928C-...</td>\n",
       "      <td>Week9/Week9_39301</td>\n",
       "      <td>Week9_39301</td>\n",
       "      <td>G11</td>\n",
       "      <td>3</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DMSO</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3848 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      table_nr  ImageNumber  \\\n",
       "0            1            9   \n",
       "1            1           10   \n",
       "2            1           11   \n",
       "3            1           12   \n",
       "4            1          249   \n",
       "...        ...          ...   \n",
       "3843         9         3560   \n",
       "3844         9         3597   \n",
       "3845         9         3598   \n",
       "3846         9         3599   \n",
       "3847         9         3600   \n",
       "\n",
       "                                           filename_dna        plate  \\\n",
       "0     Week1_150607_B04_s1_w11323931B-BDA7-4F42-870E-...  Week1_22123   \n",
       "1     Week1_150607_B04_s2_w1F649C703-6FA8-406F-8575-...  Week1_22123   \n",
       "2     Week1_150607_B04_s3_w135D66B4C-0548-4AB8-A57B-...  Week1_22123   \n",
       "3     Week1_150607_B04_s4_w1EB720AD1-83BA-48A7-8C47-...  Week1_22123   \n",
       "4     Week1_150607_B04_s1_w17D5A011C-EA14-4EA7-A62C-...  Week1_22141   \n",
       "...                                                 ...          ...   \n",
       "3843  Week9_090907_F11_s4_w19580FF4D-DC3D-4BD0-93FE-...  Week9_39301   \n",
       "3844  Week9_090907_G11_s1_w1EDE534D2-FCEE-4F92-A30B-...  Week9_39301   \n",
       "3845  Week9_090907_G11_s2_w10B010F39-3B4B-4DCB-8E34-...  Week9_39301   \n",
       "3846  Week9_090907_G11_s3_w10394282C-6D3D-4E0E-9FA3-...  Week9_39301   \n",
       "3847  Week9_090907_G11_s4_w1C447A151-1F85-4E19-9C96-...  Week9_39301   \n",
       "\n",
       "                                       filename_tubulin  \\\n",
       "0     Week1_150607_B04_s1_w2F8F7EA7A-EC57-49CA-A556-...   \n",
       "1     Week1_150607_B04_s2_w2802CC81E-56F9-41C5-A6C2-...   \n",
       "2     Week1_150607_B04_s3_w2D81AEFB9-9DD4-4B59-9177-...   \n",
       "3     Week1_150607_B04_s4_w261B79A05-7534-46F3-8C80-...   \n",
       "4     Week1_150607_B04_s1_w20ED4F2D9-0D4A-4CA6-A13D-...   \n",
       "...                                                 ...   \n",
       "3843  Week9_090907_F11_s4_w2DC65EC6F-BDCA-4B05-B243-...   \n",
       "3844  Week9_090907_G11_s1_w26A22E27F-6A81-43F5-9587-...   \n",
       "3845  Week9_090907_G11_s2_w2720AC778-3F85-4293-8D75-...   \n",
       "3846  Week9_090907_G11_s3_w24C59DB62-E99B-4284-BAD2-...   \n",
       "3847  Week9_090907_G11_s4_w22E574F48-321D-4470-ACC4-...   \n",
       "\n",
       "     Image_PathName_Tubulin  \\\n",
       "0         Week1/Week1_22123   \n",
       "1         Week1/Week1_22123   \n",
       "2         Week1/Week1_22123   \n",
       "3         Week1/Week1_22123   \n",
       "4         Week1/Week1_22141   \n",
       "...                     ...   \n",
       "3843      Week9/Week9_39301   \n",
       "3844      Week9/Week9_39301   \n",
       "3845      Week9/Week9_39301   \n",
       "3846      Week9/Week9_39301   \n",
       "3847      Week9/Week9_39301   \n",
       "\n",
       "                                         filename_actin Image_PathName_Actin  \\\n",
       "0     Week1_150607_B04_s1_w494DCA5C4-3531-497D-A8B0-...    Week1/Week1_22123   \n",
       "1     Week1_150607_B04_s2_w4342F300D-60F8-4256-A637-...    Week1/Week1_22123   \n",
       "2     Week1_150607_B04_s3_w408BE006A-BF34-457E-81A9-...    Week1/Week1_22123   \n",
       "3     Week1_150607_B04_s4_w49FF7E7B1-F049-4994-BCA2-...    Week1/Week1_22123   \n",
       "4     Week1_150607_B04_s1_w413EE84DD-212B-4008-99A9-...    Week1/Week1_22141   \n",
       "...                                                 ...                  ...   \n",
       "3843  Week9_090907_F11_s4_w45699A0F4-9AEE-4CD4-8973-...    Week9/Week9_39301   \n",
       "3844  Week9_090907_G11_s1_w4554A2BF7-0D53-4D27-BF92-...    Week9/Week9_39301   \n",
       "3845  Week9_090907_G11_s2_w49B290958-BCF2-4DDD-B0E9-...    Week9/Week9_39301   \n",
       "3846  Week9_090907_G11_s3_w471FE25C8-2477-456F-9D74-...    Week9/Week9_39301   \n",
       "3847  Week9_090907_G11_s4_w4200C5003-7F75-47DF-928C-...    Week9/Week9_39301   \n",
       "\n",
       "     Image_Metadata_Plate_DAPI Image_Metadata_Well_DAPI  Replicate  \\\n",
       "0                  Week1_22123                      B04          1   \n",
       "1                  Week1_22123                      B04          1   \n",
       "2                  Week1_22123                      B04          1   \n",
       "3                  Week1_22123                      B04          1   \n",
       "4                  Week1_22141                      B04          2   \n",
       "...                        ...                      ...        ...   \n",
       "3843               Week9_39301                      F11          3   \n",
       "3844               Week9_39301                      G11          3   \n",
       "3845               Week9_39301                      G11          3   \n",
       "3846               Week9_39301                      G11          3   \n",
       "3847               Week9_39301                      G11          3   \n",
       "\n",
       "            compound  compound_uM               MoA  Unique_MoA  \\\n",
       "0     cytochalasin B         10.0  Actin disruptors           0   \n",
       "1     cytochalasin B         10.0  Actin disruptors           0   \n",
       "2     cytochalasin B         10.0  Actin disruptors           0   \n",
       "3     cytochalasin B         10.0  Actin disruptors           0   \n",
       "4     cytochalasin B         10.0  Actin disruptors           0   \n",
       "...              ...          ...               ...         ...   \n",
       "3843            DMSO          0.0              DMSO           0   \n",
       "3844            DMSO          0.0              DMSO           0   \n",
       "3845            DMSO          0.0              DMSO           0   \n",
       "3846            DMSO          0.0              DMSO           0   \n",
       "3847            DMSO          0.0              DMSO           0   \n",
       "\n",
       "      Unique_Treatments  Unique_Compounds  \n",
       "0                     0                 0  \n",
       "1                     0                 0  \n",
       "2                     0                 0  \n",
       "3                     0                 0  \n",
       "4                     0                 0  \n",
       "...                 ...               ...  \n",
       "3843                  0                 0  \n",
       "3844                  0                 0  \n",
       "3845                  0                 0  \n",
       "3846                  0                 0  \n",
       "3847                  0                 0  \n",
       "\n",
       "[3848 rows x 17 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shabi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "tra = [transforms.ToTensor()]\n",
    "    \n",
    "# load the data\n",
    "end = time.time()\n",
    "\n",
    "dataset = bbbc021_dataset(args.data, args.csv, 'MoA', transform=transforms.Compose(tra))\n",
    "\n",
    "if args.verbose: \n",
    "    print('Load dataset: {0:.2f} s'.format(time.time() - end))\n",
    "    \n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=args.batch,\n",
    "                                         num_workers=args.workers,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of compounds: {}'.format(len(dataset.get_df()['compound'].unique())))\n",
    "print('Number of treatments: {}'.format(len(dataset.get_df()['pseudoclass'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_df().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    img = dataset[i][0]\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img[0,:,:])\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img[1,:,:])\n",
    "    \n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    plt.imshow(img[2,:,:])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering algorithm to use\n",
    "if args.clustering == \"Kmeans\":\n",
    "    deepcluster = clustering.Kmeans(k=args.nmb_cluster, \n",
    "                                    dim_method=args.dim_method, \n",
    "                                    n_components=args.n_components,\n",
    "                                    n_jobs=args.workers+1)\n",
    "elif args.clustering == \"AdaptiveKmeans\":\n",
    "    deepcluster = clustering.AdaptiveKmeans(initial_k=args.nmb_cluster, \n",
    "                                            end_k=args.end_k, \n",
    "                                            end_epochs=args.end_epochs, \n",
    "                                            initial_epoch=args.start_epoch, \n",
    "                                            dim_method=args.dim_method, \n",
    "                                            n_components=args.n_components,\n",
    "                                            n_jobs=args.workers+1)\n",
    "elif args.clustering == \"PIC\":\n",
    "    deepcluster = clustering.PIC(sigma=args.sigma, \n",
    "                                 nnn=args.nnn, \n",
    "                                 dim_method=args.dim_method, \n",
    "                                 n_components=args.n_components,\n",
    "                                 n_jobs=args.workers+1)\n",
    "elif args.clustering == \"HDBscan\":\n",
    "    deepcluster = clustering.HDBscan(min_cluster_size=args.min_cluster_size, \n",
    "                                    min_samples=args.min_samples, \n",
    "                                    nnn=args.nnn, \n",
    "                                    dim_method=args.dim_method, \n",
    "                                    n_components=args.n_components,\n",
    "                                    n_jobs=args.workers+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "nmi_list = [0]\n",
    "best_NSC_acc = 0\n",
    "df_meta = dataset.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training convnet with DeepCluster\n",
    "start_train_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    end = time.time()\n",
    "\n",
    "    # remove head\n",
    "    model.top_layer = None\n",
    "    model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "\n",
    "    # compute features for the whole dataset\n",
    "    features = compute_features(dataloader, model, len(dataset), args)\n",
    "    #print(features.shape)\n",
    "    embeds = pd.DataFrame(features, columns=embeds_cols)\n",
    "    embeds = pd.concat([df_meta, embeds], axis=1)\n",
    "    \n",
    "    # batch correction \n",
    "    embeds = do_batch_correction(embeds, embeds_cols, args.batch_corrections, verbose=args.verbose)\n",
    "        \n",
    "    # evaluate features\n",
    "    df_eval = evaluate_epoch(embeds.copy(), embeds_cols, verbose=args.verbose)\n",
    "    \n",
    "    # best epoch\n",
    "    NSC_acc = df_eval[\"NSC_1-NN_treatment\"][0]\n",
    "    if NSC_acc > best_NSC_acc:\n",
    "        best_NSC_acc = NSC_acc\n",
    "    \n",
    "    # cluster the features\n",
    "    clustering_loss = deepcluster.cluster(embeds[embeds_cols].to_numpy(copy=True), verbose=args.verbose)\n",
    "\n",
    "    # save cluster assignment\n",
    "    df_assign_epoch = pd.DataFrame(clustering.arrange_clustering(deepcluster.images_lists), columns=['Assignment_{}'.format(epoch)])\n",
    "\n",
    "    # assign pseudo-labels\n",
    "    train_dataset = clustering.cluster_assign(deepcluster.images_lists, dataset.imgs)\n",
    "\n",
    "    # uniformely sample per target\n",
    "    sampler = UnifLabelSampler(int(args.reassign * len(train_dataset)),\n",
    "                               deepcluster.images_lists)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch,\n",
    "        num_workers=args.workers,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # set last fully connected layer\n",
    "    mlp = list(model.classifier.children())\n",
    "    mlp.append(nn.ReLU(inplace=True).cuda())\n",
    "    model.classifier = nn.Sequential(*mlp)\n",
    "    model.top_layer = nn.Linear(args.n_features, len(deepcluster.images_lists))\n",
    "    model.top_layer.weight.data.normal_(0, 0.01)\n",
    "    model.top_layer.bias.data.zero_()\n",
    "    model.top_layer.cuda()\n",
    "\n",
    "    # train network with clusters as pseudo-labels\n",
    "    start_epoch_train_time = time.time()\n",
    "    loss = train(train_dataloader, model, criterion, optimizer, epoch, args)\n",
    "    loss_list.append(float(loss))\n",
    "\n",
    "    # print log\n",
    "    if args.verbose:\n",
    "        print('###### Epoch [{0}] ###### \\n'\n",
    "              'Total Time: {1:.3f} s\\n'\n",
    "              'Training time: {2:.3f} s\\n'\n",
    "              'ConvNet loss: {3:.3f} \\n'\n",
    "              'NSC acc: {4:.3f} \\n'\n",
    "              'Silhoutte score: {5:.3f} \\n'\n",
    "              'Best NSC acc: {6:.3f}'\n",
    "              .format(epoch, \n",
    "                      time.time() - end, \n",
    "                      time.time() - start_epoch_train_time, \n",
    "                      loss, \n",
    "                      df_eval[\"NSC_1-NN_treatment\"][0]*100, \n",
    "                      df_eval[\"silhou_moa\"][0],\n",
    "                      best_NSC_acc*100\n",
    "                     ))\n",
    "        try:\n",
    "            if epoch > 0:\n",
    "                nmi = normalized_mutual_info_score(\n",
    "                    clustering.arrange_clustering(deepcluster.images_lists),\n",
    "                    clustering.arrange_clustering(cluster_log.data['clustering'][-1])\n",
    "                )\n",
    "                print('NMI against previous assignment: {0:.3f}'.format(nmi))\n",
    "            else:\n",
    "                nmi = 0\n",
    "            nmi_list.append(nmi)\n",
    "        except IndexError:\n",
    "            pass\n",
    "        print('####################### \\n')\n",
    "    # save running checkpoint\n",
    "    torch.save({'epoch': epoch + 1,\n",
    "                'arch': args.arch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer' : optimizer.state_dict()},\n",
    "               os.path.join(args.output_path, 'checkpoint.pth.tar'))\n",
    "\n",
    "    # save cluster assignments, evaluation data and features\n",
    "    cluster_log.log(epoch, embeds, df_eval, df_assign_epoch, loss, nmi, deepcluster.images_lists)\n",
    "    \n",
    "    # write to tensorboard\n",
    "    if tensorboard_writer is not None:\n",
    "        tensorboard_writer.add_scalar('loss', loss, epoch)\n",
    "        tensorboard_writer.add_scalar('nmi', nmi, epoch)\n",
    "        for eval_metric, data in df_eval.iteritems():\n",
    "            tensorboard_writer.add_scalar(eval_metric, data[0], epoch)\n",
    "        \n",
    "print('Total training time {}'.format(time.time() - start_train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load training data (for loading a completed training)\n",
    "cluster_log.load_data(args.resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_log.data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_log.data['features'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric = pd.concat(cluster_log.data['eval_metrices'], axis=0)\n",
    "df_metric[\"epoch\"] = cluster_log.data['epoch']\n",
    "df_metric[\"loss\"] = cluster_log.data['loss']\n",
    "df_metric[\"loss\"] = df_metric[\"loss\"].map(float)\n",
    "df_metric[\"nmi\"] = cluster_log.data['nmi']\n",
    "df_metric[\"nmi\"] = df_metric[\"nmi\"].map(float)\n",
    "df_metric = df_metric.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric = df_metric.rename(columns={'n_clusters_well':'n_clusters'})\n",
    "df_metric['total_score'] = (df_metric['adj-mut_comp_pred'] + df_metric['comple_treat-pred'] + df_metric['silhou_pred_tsne'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_summary(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal partitional validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inter_part_validation(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal hierarchical validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inter_hier_validation(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_external_validation(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_NSC_NSB(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_total_score(df_metric, args.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best epoch for labeled and unlabeled situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best epoch labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_labeled_silh = df_metric['silhou_moa_tsne'].idxmax()\n",
    "print('Best epoch (Silhouette): ' , best_epoch_labeled_silh)\n",
    "print('NSC: ', df_metric[\"NSC_1-NN_treatment\"][best_epoch_labeled_silh])\n",
    "print('Silhouette moa tsne: ', df_metric['silhou_moa_tsne'][best_epoch_labeled_silh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_labeled_nsc = df_metric['NSC_1-NN_treatment'].idxmax()\n",
    "print('Best epoch (NSC): ' , best_epoch_labeled_nsc)\n",
    "print('NSC: ', df_metric[\"NSC_1-NN_treatment\"][best_epoch_labeled_nsc])\n",
    "print('Silhouette moa tsne: ', df_metric['silhou_moa_tsne'][best_epoch_labeled_nsc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best epoch unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_unlabeled = df_metric.loc[:,'total_score'].idxmax()\n",
    "print('Best epoch (unlabeled): ' , best_epoch_unlabeled)\n",
    "print('NSC: ', df_metric['NSC_1-NN_treatment'][best_epoch_unlabeled])\n",
    "print('Silhouette pred tsne: ', df_metric['silhou_pred_tsne'][best_epoch_unlabeled])\n",
    "print('Adjusted Rand index (treat-pred): ', df_metric['adj-rand_treat-pred'][best_epoch_unlabeled])\n",
    "print('Adjusted Mutual information (treat-pred): ', df_metric['adj-mut_treat-pred'][best_epoch_unlabeled])\n",
    "print('Adjusted Mutual information (moa-pred): ', df_metric['adj-mut_moa-pred'][best_epoch_unlabeled])\n",
    "print('Jaccard (treat-pred): ', df_metric['jaccard_treat-pred'][best_epoch_unlabeled])\n",
    "print('Completeness (treat-pred): ', df_metric['comple_treat-pred'][best_epoch_unlabeled])\n",
    "print('Number of clusters: ', df_metric['n_clusters'][best_epoch_unlabeled])\n",
    "print('Total score: ', df_metric['total_score'][best_epoch_unlabeled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save hyperparameter with best values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tensorboard_writer is not None:\n",
    "    tensorboard_writer.add_hparams(hparam_dict={\n",
    "                                    'cuda_version': args.cuda,\n",
    "                                    'lr': args.lr, \n",
    "                                    'batch_size': args.batch, \n",
    "                                    'n_features': args.n_features,\n",
    "                                    'batch_correction': batch_corrections_parameter,\n",
    "                                    'dim_method': args.dim_method, \n",
    "                                    'n_features_reduced': args.n_components,\n",
    "                                    'clustering_algorithmn': args.clustering,\n",
    "                                    'clustering_parameter': clustering_parameter,\n",
    "        \n",
    "                                   }, \n",
    "                                   metric_dict={\n",
    "                                       # labeled best silhoutte score moa\n",
    "                                       'labeled_best_silh/epoch': best_epoch_labeled_silh,\n",
    "                                       'labeled_best_silh/silhou_moa_tsne': df_metric['silhou_moa_tsne'][best_epoch_labeled_silh],\n",
    "                                       'labeled_best_silh/NSC_1-NN_treatment': df_metric['NSC_1-NN_treatment'][best_epoch_labeled_silh],\n",
    "                                       'labeled_best_silh/NSC_1-NN_avg': df_metric['NSC_1-NN_avg'][best_epoch_labeled_silh],\n",
    "                                       # labeled NSC\n",
    "                                       'labeled_best_nsc/epoch': best_epoch_labeled_nsc,\n",
    "                                       'labeled_best_nsc/silhou_moa_tsne': df_metric['silhou_moa_tsne'][best_epoch_labeled_nsc],\n",
    "                                       'labeled_best_nsc/NSC_1-NN_treatment': df_metric['NSC_1-NN_treatment'][best_epoch_labeled_nsc],\n",
    "                                       'labeled_best_nsc/NSC_1-NN_avg': df_metric['NSC_1-NN_avg'][best_epoch_labeled_nsc],\n",
    "                                       # unlabeled\n",
    "                                       'unlabeled/epoch': best_epoch_unlabeled,\n",
    "                                       'unlabeled/NSC_1-NN_treatment': df_metric['NSC_1-NN_treatment'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/silhou_moa_tsne': df_metric['silhou_moa_tsne'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/silhou_pred_tsne': df_metric['silhou_pred_tsne'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/adj-rand_treat-pred': df_metric['adj-rand_treat-pred'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/adj-mut_treat-pred': df_metric['adj-mut_treat-pred'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/jaccard_treat-pred': df_metric['jaccard_treat-pred'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/comple_treat-pred': df_metric['comple_treat-pred'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/n_clusters': df_metric['n_clusters'][best_epoch_unlabeled],\n",
    "                                       'unlabeled/total_score': df_metric['total_score'][best_epoch_unlabeled]\n",
    "                                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric.to_csv(metrices_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = dataset.get_df()\n",
    "df_meta.to_csv(metadata_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_list = cluster_log.data['cluster_assignments']\n",
    "df_assignments = pd.concat(assign_list, axis=1)\n",
    "df_assignments = pd.concat([df_meta, df_assignments], axis=1)\n",
    "df_assignments.to_csv(assign_epoch_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame({'model_loss':cluster_log.data['loss'], \n",
    "                        'NMI':cluster_log.data['nmi']})\n",
    "loss_df.to_csv(loss_path, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the best embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_labeled_sil_embeds = cluster_log.data['features'][best_epoch_labeled_silh]\n",
    "df_best_labeled_nsc_embeds = cluster_log.data['features'][best_epoch_labeled_nsc]\n",
    "df_best_unlabeled_embeds = cluster_log.data['features'][best_epoch_unlabeled]\n",
    "\n",
    "df_best_labeled_sil_embeds.to_csv(best_labeled_sil_embed_path, sep='\\t', index=False)\n",
    "df_best_labeled_nsc_embeds.to_csv(best_labeled_nsc_embed_path, sep='\\t', index=False)\n",
    "df_best_unlabeled_embeds.to_csv(best_unlabeled_embed_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best epoch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_labeled_sil_save_well = evaluate_training(df_best_labeled_sil_embeds, embeds_cols, os.path.join(args.output_path,'best_labeled_sil'), verbose=args.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_labeled_ncs_save_well = evaluate_training(df_best_labeled_nsc_embeds, embeds_cols, os.path.join(args.output_path,'best_labeled_nsc'), verbose=args.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_best_unlabeled_save_well = evaluate_training(df_best_unlabeled_embeds, embeds_cols, os.path.join(args.output_path,'best_unlabeled'), verbose=args.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umm",
   "language": "python",
   "name": "umm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
